{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#OSMI-Health-Survey-2016:-Inference\" data-toc-modified-id=\"OSMI-Health-Survey-2016:-Inference-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>OSMI Health Survey 2016: Inference</a></div><div class=\"lev1 toc-item\"><a href=\"#Recap\" data-toc-modified-id=\"Recap-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Recap</a></div><div class=\"lev1 toc-item\"><a href=\"#Fit-and-Diagnostics\" data-toc-modified-id=\"Fit-and-Diagnostics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fit and Diagnostics</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSMI Health Survey 2016: Inference\n",
    "\n",
    "_By [Michael Rosenberg](mailto:mmrosenb@andrew.cmu.edu)._\n",
    "\n",
    "_**Description**: Contains my inference related to the [OSMI Mental Health In Tech Survey 2016](https://osmihelp.org/research/). This notebook is written in `R`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "#constants\n",
    "sigLev = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "As discussed in my [model selection procedure](modelSelection.ipynb), I ended up choosing a model that represents a logistic regression, i.e.\n",
    "\n",
    "$$P(diagnosedWithMHD_i | X) = \\frac{1}{1+e^{-\\hat{r}(X)}},$$\n",
    "\n",
    "where $\\hat{r}(X)$ is the fitted regression function that contains the following variables:\n",
    "\n",
    "* ```age```: covers the age of a respondent.\n",
    "\n",
    "* ```roleType```: engages the role type of a respondent at work, which can either be technical, non-technical, or both (i.e. hybrid roles).\n",
    "\n",
    "* ```isUSA```: asks if the individual works in the USA or not (1 if they do, 0 if they don't).\n",
    "\n",
    "* ```gender```: the gender that the individual identifies with. For the sake of simplification, we say that an individual is give \"F\" if they identify as a female, \"M\" if they identify as a male, and \"O\" if they identify as another gender not along the binary. We had to do this pooling for \"O\" since there were unfortunately only a small number of observations that did not identify along the gender binary.\n",
    "\n",
    "* The interaction term between ```age``` and ```gender```.\n",
    "\n",
    "Let us fit this on our test set and perform some set of inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"diagnosedWithMHD ~ age+factor(gender)+factor(roleType)+factor(isUSA)+age:factor(gender)\"\n"
     ]
    }
   ],
   "source": [
    "#load in formula\n",
    "formulaFilename = \"../models/finalLogisticRegressionFormula.txt\"\n",
    "#get rid of newline character at the end, so -1\n",
    "formula = readChar(formulaFilename,file.info(formulaFilename)$size - 1)\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in data\n",
    "inferenceSet = read.csv(\"../data/processed/test.csv\")\n",
    "#then fit model\n",
    "finalMod.logr = glm(formula,data = inferenceSet,family = \"binomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how well our model is fitting the inference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The proportion accurate on the inference set is 0.604\"\n"
     ]
    }
   ],
   "source": [
    "inferenceSet$prediction = predict(finalMod.logr,type = \"response\")\n",
    "#make decision rule\n",
    "decRule = .5\n",
    "inferenceSet$prediction = ifelse(inferenceSet$prediction > decRule,1,0)\n",
    "#make comparision\n",
    "correctFrame = inferenceSet[which(inferenceSet$diagnosedWithMHD ==\n",
    "                                  inferenceSet$prediction),]\n",
    "#get proportion accurate\n",
    "propAccurate = dim(correctFrame)[1] / dim(inferenceSet)[1]\n",
    "print(paste(\"The proportion accurate on the inference set is\",signif(\n",
    "                                                    propAccurate,sigLev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still not an amazing fit, but it's performing about on par as the fits in our previous discussion. This may suggest that we simply aren't fitting the data extremely well, and that it might be essential to go back into the survey and find other variables that would be strong predictors of this outcome. We could also be dealing with the question of simply not asking all the questions we need to get a full picture of someone's mental health (e.g. how are there eating habits, what are there hours like at work, what is their social life like, etc). We may be able to get some more meaningful statements about factors contributing to mental health in tech if we asked some of these questions.\n",
    "\n",
    "Let's see by the confusion matrix what kinds of places are we making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Actual 0</th><th scope=col>Actual 1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Predict 0</th><td>233</td><td>161</td></tr>\n",
       "\t<tr><th scope=row>Predict 1</th><td>122</td><td>198</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Actual 0 & Actual 1\\\\\n",
       "\\hline\n",
       "\tPredict 0 & 233 & 161\\\\\n",
       "\tPredict 1 & 122 & 198\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "1. 233\n",
       "2. 122\n",
       "3. 161\n",
       "4. 198\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "          Actual 0 Actual 1\n",
       "Predict 0 233      161     \n",
       "Predict 1 122      198     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusionMat = matrix(0,nrow = 2,ncol = 2)\n",
    "for (i in 1:2){\n",
    "    for (j in 1:2){\n",
    "        #get level associated\n",
    "        confusionMat[i,j] = length(which(inferenceSet$predictions == i - 1 &\n",
    "                                inferenceSet$diagnosedWithMHD == j - 1))\n",
    "    }\n",
    "}\n",
    "#name columns\n",
    "rownames(confusionMat) = c(\"Predict 0\",\"Predict 1\")\n",
    "colnames(confusionMat) = c(\"Actual 0\",\"Actual 1\")\n",
    "confusionMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We see that we have a false "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
